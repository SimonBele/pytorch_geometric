{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_version = str(torch.__version__)\n",
    "# scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "# sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "# !pip install torch-scatter -f $scatter_src\n",
    "# !pip install torch-sparse -f $sparse_src\n",
    "# !pip install torch-geometric\n",
    "# !pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# import random\n",
    "# import torch.nn.functional as F\n",
    "# print(torch.__version__)\n",
    "\n",
    "# # The PyG built-in GCNConv\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# import torch_geometric.transforms as T\n",
    "\n",
    "# from torch_geometric.data import DataLoader, Data, Dataset\n",
    "# from tqdm.notebook import tqdm\n",
    "# from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "# from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "# from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "# from torch.nn import BatchNorm1d\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AcyclicGraphDataset(Dataset):\n",
    "#     def __init__(self, pyg_dataset):\n",
    "#         super(AcyclicGraphDataset, self).__init__()\n",
    "#         self.pyg_dataset = pyg_dataset\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.pyg_dataset)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         return self.pyg_dataset[idx]\n",
    "\n",
    "# class CyclicGraphDataset(Dataset):\n",
    "#     def __init__(self, data_list):\n",
    "#         super(CyclicGraphDataset, self).__init__()\n",
    "#         self.data_list = data_list\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.data_list)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         return self.data_list[idx]\n",
    "\n",
    "# cyclic_dataset = torch.load(\"cyclic_dataset.pt\")\n",
    "# acyclic_dataset = torch.load(\"acyclic_dataset.pt\")\n",
    "# print('The {} dataset has {} graphs'.format(\"cyclic\", len(cyclic_dataset)))\n",
    "# print('The {} dataset has {} graphs'.format(\"acyclic\", len(acyclic_dataset)))\n",
    "# cyclic_data = cyclic_dataset[0]\n",
    "# acyclic_data = acyclic_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# class IsAcyclic(Dataset):\n",
    "#     def __init__(self, cyclic_data, acyclic_data):\n",
    "#         super(IsAcyclic, self).__init__()\n",
    "#         self.cyclic_data = cyclic_data\n",
    "#         self.acyclic_data = acyclic_data\n",
    "#         # Combine the two datasets\n",
    "        \n",
    "#         self.data_list = [(data, 0) for data in cyclic_data] + [(data, 1) for data in acyclic_data]\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.data_list)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         data, label = self.data_list[idx]\n",
    "#         # Ensure the label is a tensor and attach it to the data object\n",
    "#         data.y = torch.tensor([label], dtype=torch.float)\n",
    "#         return data\n",
    "\n",
    "#     def get_idx_split(self, train_ratio=0.7, val_ratio=0.15):\n",
    "#         def split_indices(data, train_ratio, val_ratio):\n",
    "#             dataset_size = len(data)\n",
    "#             indices = list(range(dataset_size))\n",
    "#             random.shuffle(indices)\n",
    "\n",
    "#             train_split = int(train_ratio * dataset_size)\n",
    "#             val_split = int(val_ratio * dataset_size) + train_split\n",
    "\n",
    "#             return indices[:train_split], indices[train_split:val_split], indices[val_split:]\n",
    "\n",
    "#         # Split cyclic and acyclic datasets separately\n",
    "#         cyclic_train, cyclic_val, cyclic_test = split_indices(self.cyclic_data, train_ratio, val_ratio)\n",
    "#         acyclic_train, acyclic_val, acyclic_test = split_indices(self.acyclic_data, train_ratio, val_ratio)\n",
    "\n",
    "#         # Offset acyclic indices by the size of cyclic dataset\n",
    "#         offset = len(self.cyclic_data)\n",
    "#         acyclic_train = [i + offset for i in acyclic_train]\n",
    "#         acyclic_val = [i + offset for i in acyclic_val]\n",
    "#         acyclic_test = [i + offset for i in acyclic_test]\n",
    "\n",
    "#         # Combine the splits from cyclic and acyclic datasets\n",
    "#         train_indices = cyclic_train + acyclic_train\n",
    "#         val_indices = cyclic_val + acyclic_val\n",
    "#         test_indices = cyclic_test + acyclic_test\n",
    "\n",
    "#         # Shuffle combined splits to mix cyclic and acyclic graphs\n",
    "#         random.shuffle(train_indices)\n",
    "#         random.shuffle(val_indices)\n",
    "#         random.shuffle(test_indices)\n",
    "\n",
    "#         return {\n",
    "#             'train': train_indices,\n",
    "#             'valid': val_indices,\n",
    "#             'test': test_indices\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Assuming 'cyclic_dataset' and 'acyclic_dataset' are already created as per your provided code\n",
    "# dataset = IsAcyclic(cyclic_dataset, acyclic_dataset)\n",
    "\n",
    "# torch.save(dataset, 'is_acyclic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_idx = dataset.get_idx_split()\n",
    "\n",
    "# train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
    "# valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
    "# test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'device': device,\n",
    "#     'input_dim' : 1,\n",
    "#     'gcn_output_dim' : [8, 16],\n",
    "#     'dropout': 0.5,\n",
    "#     'lr': 0.01,\n",
    "#     'weight_decay' : 0.00001,\n",
    "#     'epochs': 30,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the node degrees as the initial features for all nodes.\n",
    "# Then we apply two layers of GCNs with output dimensions\n",
    "# equal to 8, 16 respectively and perform global averaging to obtain\n",
    "# the graph representations. Finally, we employ one fully-connected\n",
    "# layer as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, gcn_output_dims, dropout, return_embeds=False):\n",
    "#         super(GCN, self).__init__()\n",
    "\n",
    "#         # A list of GCNConv layers\n",
    "#         self.convs = None\n",
    "\n",
    "#         # A list of 1D batch normalization layers\n",
    "#         self.bns = None\n",
    "\n",
    "#         # The log softmax layer\n",
    "#         self.softmax = None\n",
    "\n",
    "#         self.convs = torch.nn.ModuleList([GCNConv(in_channels=input_dim, out_channels=gcn_output_dims[0])])\n",
    "#         self.convs.extend([GCNConv(in_channels=gcn_output_dims[i + 0], out_channels=gcn_output_dims[i + 1]) for i in range(len(gcn_output_dims) - 1)])\n",
    "\n",
    "#         self.bns = torch.nn.ModuleList([BatchNorm1d(num_features=gcn_output_dims[l]) for l in range(len(gcn_output_dims) - 1)])\n",
    "        \n",
    "#         self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "#         # Probability of an element getting zeroed\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         # Skip classification layer and return node embeddings\n",
    "#         self.return_embeds = return_embeds\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         for conv in self.convs:\n",
    "#             conv.reset_parameters()\n",
    "#         for bn in self.bns:\n",
    "#             bn.reset_parameters()\n",
    "\n",
    "#     def forward(self, x, adj_t):\n",
    "#         out = None\n",
    "\n",
    "#         for i in range(len(self.convs)-1):\n",
    "#           x = F.relu(self.bns[i](self.convs[i](x, adj_t)))\n",
    "#           if self.training:\n",
    "#             x = F.dropout(x, p=self.dropout)\n",
    "#         x = self.convs[-1](x, adj_t)\n",
    "#         if self.return_embeds:\n",
    "#           out = x\n",
    "#         else:\n",
    "#           out = self.softmax(x)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# ### GCN to predict graph property\n",
    "# class GCN_Graph(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, gcn_output_dims, output_dim, dropout):\n",
    "#         super(GCN_Graph, self).__init__()\n",
    "\n",
    "#         # self.node_encoder = AtomEncoder(hidden_dim)\n",
    "        \n",
    "#         self.gnn_node = GCN(input_dim, gcn_output_dims, dropout, return_embeds=True)\n",
    "\n",
    "#         self.pool = global_mean_pool # global averaging to obtain graph representation\n",
    "\n",
    "#         # Output layer\n",
    "#         self.linear = torch.nn.Linear(gcn_output_dims[-1], output_dim) # One fully connected layer as a classifier\n",
    "\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#       self.gnn_node.reset_parameters()\n",
    "#       self.linear.reset_parameters()\n",
    "\n",
    "#     def forward(self, batched_data):\n",
    "#         # Extract important attributes of our mini-batch\n",
    "#         x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        \n",
    "#         device = edge_index.device\n",
    "#         degrees = torch.sum(edge_index[0] == torch.arange(edge_index.max() + 1, device=device)[:, None], dim=1, dtype=torch.float)\n",
    "#         x = degrees.unsqueeze(1)  # Add feature dimension\n",
    "#         embed = x.to(device)  # Ensure the embedding tensor is on the correct device\n",
    "\n",
    "#         out = None\n",
    "\n",
    "#         node_embeddings = self.gnn_node(embed, edge_index)\n",
    "#         agg_features = self.pool(node_embeddings, batch)\n",
    "#         out = self.linear(agg_features)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# def train(model, device, data_loader, optimizer, loss_fn):\n",
    "#     model.train()\n",
    "#     loss = 0\n",
    "\n",
    "#     for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "#       batch = batch.to(device)\n",
    "\n",
    "#       if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "#           pass\n",
    "#       else:\n",
    "#         ## ignore nan targets (unlabeled) when computing training loss.\n",
    "#         is_labeled = batch.y == batch.y\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(batch)\n",
    "#         filtered_output = out[is_labeled]\n",
    "\n",
    "#         # Reshape the labels to match the output shape\n",
    "#         filtered_labels = batch.y[is_labeled].unsqueeze(1).type(torch.float32)\n",
    "\n",
    "#         loss = loss_fn(filtered_output, filtered_labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     return loss.item()\n",
    "\n",
    "# def compute_accuracy(y_true, y_pred):\n",
    "#     # Assuming y_pred are logits; apply sigmoid and round off to get binary predictions\n",
    "#     preds = torch.sigmoid(y_pred) > 0.5\n",
    "#     correct = preds.eq(y_true.view_as(preds)).sum()\n",
    "#     accuracy = correct.float() / y_true.numel()\n",
    "#     return accuracy.item()\n",
    "\n",
    "# def eval(model, device, loader):\n",
    "#     model.eval()\n",
    "#     total_accuracy = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     for batch in loader:\n",
    "#         batch = batch.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             pred = model(batch)\n",
    "\n",
    "#         # Assuming binary classification and batch.y is your ground truth\n",
    "#         accuracy = compute_accuracy(batch.y, pred)\n",
    "#         total_accuracy += accuracy * batch.y.size(0)\n",
    "#         total_samples += batch.y.size(0)\n",
    "\n",
    "#     return total_accuracy / total_samples\n",
    "\n",
    "# model = GCN_Graph(args['input_dim'], args['gcn_output_dim'],\n",
    "#             output_dim=1, dropout=args['dropout']).to(device)\n",
    "# # evaluator = Evaluator(name='ogbg-molhiv')\n",
    "\n",
    "# model.reset_parameters()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# best_model = None\n",
    "# best_valid_acc = 0\n",
    "\n",
    "# # Training loop remains the same...\n",
    "\n",
    "# # Evaluation in your main loop\n",
    "# for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "#     print('Training...')\n",
    "#     train_loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "#     print('Evaluating...')\n",
    "#     train_acc = eval(model, device, train_loader)\n",
    "#     val_acc = eval(model, device, valid_loader)\n",
    "#     test_acc = eval(model, device, test_loader)\n",
    "\n",
    "#     if val_acc > best_valid_acc:\n",
    "#         best_valid_acc = val_acc\n",
    "#         best_model = copy.deepcopy(model)\n",
    "\n",
    "#     print(f'Epoch: {epoch:02d}, '\n",
    "#           f'Loss: {train_loss:.4f}, '\n",
    "#           f'Train Acc: {100 * train_acc:.2f}%, '\n",
    "#           f'Valid Acc: {100 * val_acc:.2f}% '\n",
    "#           f'Test Acc: {100 * test_acc:.2f}%')\n",
    "\n",
    "# # Evaluate the best model\n",
    "# best_train_acc = eval(best_model, device, train_loader)\n",
    "# best_val_acc = eval(best_model, device, valid_loader)\n",
    "# best_test_acc = eval(best_model, device, test_loader)\n",
    "\n",
    "# print(f'Best model: '\n",
    "#       f'Train: {100 * best_train_acc:.2f}%, '\n",
    "#       f'Valid: {100 * best_val_acc:.2f}% '\n",
    "#       f'Test: {100 * best_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout, return_embeds=False):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(in_channels = input_dim, out_channels = 32),\n",
    "                                          GCNConv(in_channels = 32,        out_channels = 48),\n",
    "                                          GCNConv(in_channels = 48,        out_channels = 64)])\n",
    "\n",
    "        # self.bns = torch.nn.ModuleList([BatchNorm1d(num_features = gcn_output_dims[l]) for l in range(len(gcn_output_dims) - 1)])\n",
    "        \n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        out = None\n",
    "\n",
    "        for i in range(len(self.convs)):\n",
    "            x = self.convs[i](x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if self.return_embeds:\n",
    "          out = x\n",
    "        else:\n",
    "          out = self.softmax(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "### GCN to predict graph property\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, emb = False):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.gnn_node = GCN(input_dim, dropout, return_embeds=True)\n",
    "\n",
    "        self.pool = global_mean_pool # global averaging to obtain graph representation\n",
    "        \n",
    "        self.post_mp = torch.nn.Sequential(torch.nn.Linear(64, 32),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           torch.nn.Linear(32, output_dim))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      self.gnn_node.reset_parameters()\n",
    "      self.post_mp.apply(reset_parameters)\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # Extract important attributes of our mini-batch\n",
    "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        embed = x.to(device)  # Ensure the embedding tensor is on the correct device\n",
    "\n",
    "        out = None\n",
    "\n",
    "        node_embeddings = self.gnn_node(embed, edge_index)\n",
    "        x = self.pool(node_embeddings, batch)\n",
    "        x = self.post_mp(x)\n",
    "        if self.emb == True:\n",
    "            return x    \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "Device: cpu\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonbele/miniconda3/envs/research/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab719e7a717454ead05f03be4373c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?Epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum test set accuracy: 0.34210526315789475\n",
      "Minimum loss: 0.6931471824645996\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.data import DataLoader, Data, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch.utils.data import random_split\n",
    "import copy\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import copy\n",
    "from tqdm.auto import trange\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "def reset_parameters(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.reset_parameters()\n",
    "        \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout, return_embeds=False):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(in_channels = input_dim, out_channels = 32),\n",
    "                                          GCNConv(in_channels = 32,        out_channels = 48),\n",
    "                                          GCNConv(in_channels = 48,        out_channels = 64)])\n",
    "\n",
    "        # self.bns = torch.nn.ModuleList([BatchNorm1d(num_features = gcn_output_dims[l]) for l in range(len(gcn_output_dims) - 1)])\n",
    "        \n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        out = None\n",
    "\n",
    "        for i in range(len(self.convs)):\n",
    "            x = self.convs[i](x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if self.return_embeds:\n",
    "          out = x\n",
    "        else:\n",
    "          out = self.softmax(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "### GCN to predict graph property\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, emb = False):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.gnn_node = GCN(input_dim, dropout, return_embeds=True)\n",
    "\n",
    "        self.pool = global_mean_pool # global averaging to obtain graph representation\n",
    "        \n",
    "        self.post_mp = torch.nn.Sequential(torch.nn.Linear(64, 32),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           torch.nn.Linear(32, output_dim))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      self.gnn_node.reset_parameters()\n",
    "      self.post_mp.apply(reset_parameters)\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # Extract important attributes of our mini-batch\n",
    "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        embed = x.to(device)  # Ensure the embedding tensor is on the correct device\n",
    "\n",
    "        out = None\n",
    "\n",
    "        node_embeddings = self.gnn_node(embed, edge_index)\n",
    "        x = self.pool(node_embeddings, batch)\n",
    "        x = self.post_mp(x)\n",
    "        if self.emb == True:\n",
    "            return x    \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer\n",
    "\n",
    "def train(dataset, args, train_indices, val_indices, test_indices):\n",
    "    # Split dataset into training, validation, and testing\n",
    "    train_dataset = [dataset[i] for i in train_indices]\n",
    "    val_dataset = [dataset[i] for i in val_indices]\n",
    "    test_dataset = [dataset[i] for i in test_indices]\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = GCN_Graph(args.input_dim, output_dim=1, dropout=args.dropout).to(device)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    val_accs = 0\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    print(args.epochs)\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epoch\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            # Reshape output to match target labels\n",
    "            pred = pred.squeeze()\n",
    "            label = batch.y.float()\n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        # Test accuracy\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "          \n",
    "    return test_accs, val_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "        correct += pred.eq(label).sum().item()\n",
    "        total += data.num_graphs\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "args = {'batch_size': 32, \n",
    "        'dropout': 0.5, \n",
    "        'epochs': 500, \n",
    "        'input_dim' : 7,\n",
    "        'opt': 'adam', \n",
    "        'opt_scheduler': 'none', \n",
    "        'opt_restart': 0, \n",
    "        'weight_decay': 5e-3, \n",
    "        'lr': 0.01}\n",
    "\n",
    "args = objectview(args)\n",
    "\n",
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "num_graphs = len(dataset)\n",
    "\n",
    "# Define split percentages\n",
    "train_percentage = 0.8\n",
    "val_percentage = 0.0\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(num_graphs * train_percentage)\n",
    "val_size = int(num_graphs * val_percentage)\n",
    "test_size = num_graphs - train_size - val_size\n",
    "\n",
    "# Create shuffled indices\n",
    "indices = np.random.permutation(num_graphs)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "test_accs, val_accs, losses, best_model, best_acc, test_loader = train(dataset, args, train_indices, val_indices, test_indices)\n",
    "\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# Run test for our best model to save the predictions!\n",
    "test(test_loader, best_model)\n",
    "print()\n",
    "\n",
    "plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\")\n",
    "plt.plot(test_accs, label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54a8a4b80f4f70c84f31aca617cda6666e967fece7258fbe31fedfa48a0f63f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
