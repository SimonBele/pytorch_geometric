{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_version = str(torch.__version__)\n",
    "# scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "# sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "# !pip install torch-scatter -f $scatter_src\n",
    "# !pip install torch-sparse -f $sparse_src\n",
    "# !pip install torch-geometric\n",
    "# !pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# import random\n",
    "# import torch.nn.functional as F\n",
    "# print(torch.__version__)\n",
    "\n",
    "# # The PyG built-in GCNConv\n",
    "# from torch_geometric.nn import GCNConv\n",
    "\n",
    "# import torch_geometric.transforms as T\n",
    "\n",
    "# from torch_geometric.data import DataLoader, Data, Dataset\n",
    "# from tqdm.notebook import tqdm\n",
    "# from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "# from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "# from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "# from torch.nn import BatchNorm1d\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AcyclicGraphDataset(Dataset):\n",
    "#     def __init__(self, pyg_dataset):\n",
    "#         super(AcyclicGraphDataset, self).__init__()\n",
    "#         self.pyg_dataset = pyg_dataset\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.pyg_dataset)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         return self.pyg_dataset[idx]\n",
    "\n",
    "# class CyclicGraphDataset(Dataset):\n",
    "#     def __init__(self, data_list):\n",
    "#         super(CyclicGraphDataset, self).__init__()\n",
    "#         self.data_list = data_list\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.data_list)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         return self.data_list[idx]\n",
    "\n",
    "# cyclic_dataset = torch.load(\"cyclic_dataset.pt\")\n",
    "# acyclic_dataset = torch.load(\"acyclic_dataset.pt\")\n",
    "# print('The {} dataset has {} graphs'.format(\"cyclic\", len(cyclic_dataset)))\n",
    "# print('The {} dataset has {} graphs'.format(\"acyclic\", len(acyclic_dataset)))\n",
    "# cyclic_data = cyclic_dataset[0]\n",
    "# acyclic_data = acyclic_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# class IsAcyclic(Dataset):\n",
    "#     def __init__(self, cyclic_data, acyclic_data):\n",
    "#         super(IsAcyclic, self).__init__()\n",
    "#         self.cyclic_data = cyclic_data\n",
    "#         self.acyclic_data = acyclic_data\n",
    "#         # Combine the two datasets\n",
    "        \n",
    "#         self.data_list = [(data, 0) for data in cyclic_data] + [(data, 1) for data in acyclic_data]\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.data_list)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         data, label = self.data_list[idx]\n",
    "#         # Ensure the label is a tensor and attach it to the data object\n",
    "#         data.y = torch.tensor([label], dtype=torch.float)\n",
    "#         return data\n",
    "\n",
    "#     def get_idx_split(self, train_ratio=0.7, val_ratio=0.15):\n",
    "#         def split_indices(data, train_ratio, val_ratio):\n",
    "#             dataset_size = len(data)\n",
    "#             indices = list(range(dataset_size))\n",
    "#             random.shuffle(indices)\n",
    "\n",
    "#             train_split = int(train_ratio * dataset_size)\n",
    "#             val_split = int(val_ratio * dataset_size) + train_split\n",
    "\n",
    "#             return indices[:train_split], indices[train_split:val_split], indices[val_split:]\n",
    "\n",
    "#         # Split cyclic and acyclic datasets separately\n",
    "#         cyclic_train, cyclic_val, cyclic_test = split_indices(self.cyclic_data, train_ratio, val_ratio)\n",
    "#         acyclic_train, acyclic_val, acyclic_test = split_indices(self.acyclic_data, train_ratio, val_ratio)\n",
    "\n",
    "#         # Offset acyclic indices by the size of cyclic dataset\n",
    "#         offset = len(self.cyclic_data)\n",
    "#         acyclic_train = [i + offset for i in acyclic_train]\n",
    "#         acyclic_val = [i + offset for i in acyclic_val]\n",
    "#         acyclic_test = [i + offset for i in acyclic_test]\n",
    "\n",
    "#         # Combine the splits from cyclic and acyclic datasets\n",
    "#         train_indices = cyclic_train + acyclic_train\n",
    "#         val_indices = cyclic_val + acyclic_val\n",
    "#         test_indices = cyclic_test + acyclic_test\n",
    "\n",
    "#         # Shuffle combined splits to mix cyclic and acyclic graphs\n",
    "#         random.shuffle(train_indices)\n",
    "#         random.shuffle(val_indices)\n",
    "#         random.shuffle(test_indices)\n",
    "\n",
    "#         return {\n",
    "#             'train': train_indices,\n",
    "#             'valid': val_indices,\n",
    "#             'test': test_indices\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Assuming 'cyclic_dataset' and 'acyclic_dataset' are already created as per your provided code\n",
    "# dataset = IsAcyclic(cyclic_dataset, acyclic_dataset)\n",
    "\n",
    "# torch.save(dataset, 'is_acyclic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print('Device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_idx = dataset.get_idx_split()\n",
    "\n",
    "# train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
    "# valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
    "# test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'device': device,\n",
    "#     'input_dim' : 1,\n",
    "#     'gcn_output_dim' : [8, 16],\n",
    "#     'dropout': 0.5,\n",
    "#     'lr': 0.01,\n",
    "#     'weight_decay' : 0.00001,\n",
    "#     'epochs': 30,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the node degrees as the initial features for all nodes.\n",
    "# Then we apply two layers of GCNs with output dimensions\n",
    "# equal to 8, 16 respectively and perform global averaging to obtain\n",
    "# the graph representations. Finally, we employ one fully-connected\n",
    "# layer as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, gcn_output_dims, dropout, return_embeds=False):\n",
    "#         super(GCN, self).__init__()\n",
    "\n",
    "#         # A list of GCNConv layers\n",
    "#         self.convs = None\n",
    "\n",
    "#         # A list of 1D batch normalization layers\n",
    "#         self.bns = None\n",
    "\n",
    "#         # The log softmax layer\n",
    "#         self.softmax = None\n",
    "\n",
    "#         self.convs = torch.nn.ModuleList([GCNConv(in_channels=input_dim, out_channels=gcn_output_dims[0])])\n",
    "#         self.convs.extend([GCNConv(in_channels=gcn_output_dims[i + 0], out_channels=gcn_output_dims[i + 1]) for i in range(len(gcn_output_dims) - 1)])\n",
    "\n",
    "#         self.bns = torch.nn.ModuleList([BatchNorm1d(num_features=gcn_output_dims[l]) for l in range(len(gcn_output_dims) - 1)])\n",
    "        \n",
    "#         self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "#         # Probability of an element getting zeroed\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         # Skip classification layer and return node embeddings\n",
    "#         self.return_embeds = return_embeds\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         for conv in self.convs:\n",
    "#             conv.reset_parameters()\n",
    "#         for bn in self.bns:\n",
    "#             bn.reset_parameters()\n",
    "\n",
    "#     def forward(self, x, adj_t):\n",
    "#         out = None\n",
    "\n",
    "#         for i in range(len(self.convs)-1):\n",
    "#           x = F.relu(self.bns[i](self.convs[i](x, adj_t)))\n",
    "#           if self.training:\n",
    "#             x = F.dropout(x, p=self.dropout)\n",
    "#         x = self.convs[-1](x, adj_t)\n",
    "#         if self.return_embeds:\n",
    "#           out = x\n",
    "#         else:\n",
    "#           out = self.softmax(x)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# ### GCN to predict graph property\n",
    "# class GCN_Graph(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, gcn_output_dims, output_dim, dropout):\n",
    "#         super(GCN_Graph, self).__init__()\n",
    "\n",
    "#         # self.node_encoder = AtomEncoder(hidden_dim)\n",
    "        \n",
    "#         self.gnn_node = GCN(input_dim, gcn_output_dims, dropout, return_embeds=True)\n",
    "\n",
    "#         self.pool = global_mean_pool # global averaging to obtain graph representation\n",
    "\n",
    "#         # Output layer\n",
    "#         self.linear = torch.nn.Linear(gcn_output_dims[-1], output_dim) # One fully connected layer as a classifier\n",
    "\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#       self.gnn_node.reset_parameters()\n",
    "#       self.linear.reset_parameters()\n",
    "\n",
    "#     def forward(self, batched_data):\n",
    "#         # Extract important attributes of our mini-batch\n",
    "#         x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        \n",
    "#         device = edge_index.device\n",
    "#         degrees = torch.sum(edge_index[0] == torch.arange(edge_index.max() + 1, device=device)[:, None], dim=1, dtype=torch.float)\n",
    "#         x = degrees.unsqueeze(1)  # Add feature dimension\n",
    "#         embed = x.to(device)  # Ensure the embedding tensor is on the correct device\n",
    "\n",
    "#         out = None\n",
    "\n",
    "#         node_embeddings = self.gnn_node(embed, edge_index)\n",
    "#         agg_features = self.pool(node_embeddings, batch)\n",
    "#         out = self.linear(agg_features)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# def train(model, device, data_loader, optimizer, loss_fn):\n",
    "#     model.train()\n",
    "#     loss = 0\n",
    "\n",
    "#     for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "#       batch = batch.to(device)\n",
    "\n",
    "#       if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "#           pass\n",
    "#       else:\n",
    "#         ## ignore nan targets (unlabeled) when computing training loss.\n",
    "#         is_labeled = batch.y == batch.y\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(batch)\n",
    "#         filtered_output = out[is_labeled]\n",
    "\n",
    "#         # Reshape the labels to match the output shape\n",
    "#         filtered_labels = batch.y[is_labeled].unsqueeze(1).type(torch.float32)\n",
    "\n",
    "#         loss = loss_fn(filtered_output, filtered_labels)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     return loss.item()\n",
    "\n",
    "# def compute_accuracy(y_true, y_pred):\n",
    "#     # Assuming y_pred are logits; apply sigmoid and round off to get binary predictions\n",
    "#     preds = torch.sigmoid(y_pred) > 0.5\n",
    "#     correct = preds.eq(y_true.view_as(preds)).sum()\n",
    "#     accuracy = correct.float() / y_true.numel()\n",
    "#     return accuracy.item()\n",
    "\n",
    "# def eval(model, device, loader):\n",
    "#     model.eval()\n",
    "#     total_accuracy = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     for batch in loader:\n",
    "#         batch = batch.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             pred = model(batch)\n",
    "\n",
    "#         # Assuming binary classification and batch.y is your ground truth\n",
    "#         accuracy = compute_accuracy(batch.y, pred)\n",
    "#         total_accuracy += accuracy * batch.y.size(0)\n",
    "#         total_samples += batch.y.size(0)\n",
    "\n",
    "#     return total_accuracy / total_samples\n",
    "\n",
    "# model = GCN_Graph(args['input_dim'], args['gcn_output_dim'],\n",
    "#             output_dim=1, dropout=args['dropout']).to(device)\n",
    "# # evaluator = Evaluator(name='ogbg-molhiv')\n",
    "\n",
    "# model.reset_parameters()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# best_model = None\n",
    "# best_valid_acc = 0\n",
    "\n",
    "# # Training loop remains the same...\n",
    "\n",
    "# # Evaluation in your main loop\n",
    "# for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "#     print('Training...')\n",
    "#     train_loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "#     print('Evaluating...')\n",
    "#     train_acc = eval(model, device, train_loader)\n",
    "#     val_acc = eval(model, device, valid_loader)\n",
    "#     test_acc = eval(model, device, test_loader)\n",
    "\n",
    "#     if val_acc > best_valid_acc:\n",
    "#         best_valid_acc = val_acc\n",
    "#         best_model = copy.deepcopy(model)\n",
    "\n",
    "#     print(f'Epoch: {epoch:02d}, '\n",
    "#           f'Loss: {train_loss:.4f}, '\n",
    "#           f'Train Acc: {100 * train_acc:.2f}%, '\n",
    "#           f'Valid Acc: {100 * val_acc:.2f}% '\n",
    "#           f'Test Acc: {100 * test_acc:.2f}%')\n",
    "\n",
    "# # Evaluate the best model\n",
    "# best_train_acc = eval(best_model, device, train_loader)\n",
    "# best_val_acc = eval(best_model, device, valid_loader)\n",
    "# best_test_acc = eval(best_model, device, test_loader)\n",
    "\n",
    "# print(f'Best model: '\n",
    "#       f'Train: {100 * best_train_acc:.2f}%, '\n",
    "#       f'Valid: {100 * best_val_acc:.2f}% '\n",
    "#       f'Test: {100 * best_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, dropout, return_embeds=False):\n",
    "#         super(GCN, self).__init__()\n",
    "\n",
    "#         # A list of GCNConv layers\n",
    "#         self.convs = None\n",
    "\n",
    "#         # A list of 1D batch normalization layers\n",
    "#         self.bns = None\n",
    "\n",
    "#         # The log softmax layer\n",
    "#         self.softmax = None\n",
    "\n",
    "#         self.convs = torch.nn.ModuleList([GCNConv(in_channels = input_dim, out_channels = 32),\n",
    "#                                           GCNConv(in_channels = 32,        out_channels = 48),\n",
    "#                                           GCNConv(in_channels = 48,        out_channels = 64)])\n",
    "\n",
    "#         # self.bns = torch.nn.ModuleList([BatchNorm1d(num_features = gcn_output_dims[l]) for l in range(len(gcn_output_dims) - 1)])\n",
    "        \n",
    "#         self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "#         # Probability of an element getting zeroed\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         # Skip classification layer and return node embeddings\n",
    "#         self.return_embeds = return_embeds\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         for conv in self.convs:\n",
    "#             conv.reset_parameters()\n",
    "\n",
    "#     def forward(self, x, adj_t):\n",
    "#         out = None\n",
    "\n",
    "#         for i in range(len(self.convs)):\n",
    "#             x = self.convs[i](x, adj_t)\n",
    "#             x = F.relu(x)\n",
    "#             x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "#         if self.return_embeds:\n",
    "#           out = x\n",
    "#         else:\n",
    "#           out = self.softmax(x)\n",
    "\n",
    "#         return out\n",
    "\n",
    "# ### GCN to predict graph property\n",
    "# class GCN_Graph(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, dropout, emb = False):\n",
    "#         super(GCN_Graph, self).__init__()\n",
    "\n",
    "#         self.emb = emb\n",
    "#         self.gnn_node = GCN(input_dim, dropout, return_embeds=True)\n",
    "\n",
    "#         self.pool = global_mean_pool # global averaging to obtain graph representation\n",
    "        \n",
    "#         self.post_mp = torch.nn.Sequential(torch.nn.Linear(64, 32),\n",
    "#                                            torch.nn.Dropout(dropout),\n",
    "#                                            torch.nn.Linear(32, output_dim))\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#       self.gnn_node.reset_parameters()\n",
    "#       self.post_mp.apply(reset_parameters)\n",
    "\n",
    "#     def forward(self, batched_data):\n",
    "#         # Extract important attributes of our mini-batch\n",
    "#         x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "#         embed = x.to(device)  # Ensure the embedding tensor is on the correct device\n",
    "\n",
    "#         out = None\n",
    "\n",
    "#         node_embeddings = self.gnn_node(embed, edge_index)\n",
    "#         x = self.pool(node_embeddings, batch)\n",
    "#         x = self.post_mp(x)\n",
    "#         if self.emb == True:\n",
    "#             return x    \n",
    "#         return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.data import DataLoader, Data, Dataset, Batch\n",
    "from tqdm.notebook import tqdm\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch.utils.data import random_split\n",
    "import copy\n",
    "\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import copy\n",
    "from tqdm.auto import trange\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "def reset_parameters(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        module.reset_parameters()\n",
    "        \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout, return_embeds=False):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(in_channels = input_dim, out_channels = 32),\n",
    "                                          GCNConv(in_channels = 32,        out_channels = 48),\n",
    "                                          GCNConv(in_channels = 48,        out_channels = 64)])\n",
    "\n",
    "        # self.bns = torch.nn.ModuleList([BatchNorm1d(num_features = gcn_output_dims[l]) for l in range(len(gcn_output_dims) - 1)])\n",
    "        \n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        out = None\n",
    "\n",
    "        for i in range(len(self.convs)):\n",
    "            x = self.convs[i](x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        if self.return_embeds:\n",
    "          out = x\n",
    "        else:\n",
    "          out = self.softmax(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "### GCN to predict graph property\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout, emb = False):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.gnn_node = GCN(input_dim, dropout, return_embeds=True)\n",
    "\n",
    "        self.pool = global_mean_pool # global averaging to obtain graph representation\n",
    "        \n",
    "        self.post_mp = torch.nn.Sequential(torch.nn.Linear(64, 32),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           torch.nn.Linear(32, output_dim))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      self.gnn_node.reset_parameters()\n",
    "      self.post_mp.apply(reset_parameters)\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # Extract important attributes of our mini-batch\n",
    "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        embed = x.to(device)  # Ensure the embedding tensor is on the correct device\n",
    "\n",
    "        out = None\n",
    "\n",
    "        node_embeddings = self.gnn_node(embed, edge_index)\n",
    "        x = self.pool(node_embeddings, batch)\n",
    "        x \n",
    "        x = self.post_mp(x)\n",
    "        if self.emb == True:\n",
    "            return x    \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_batch(dataset):\n",
    "    data_list = [data for data in dataset]\n",
    "    batched_data = Batch.from_data_list(data_list)\n",
    "    return batched_data\n",
    "\n",
    "def train(dataset, args, train_indices, val_indices, test_indices):\n",
    "    # Split dataset into training, validation, and testing\n",
    "    train_dataset = create_single_batch([dataset[i] for i in train_indices])\n",
    "    # val_dataset = create_single_batch([dataset[i] for i in val_indices])\n",
    "    test_dataset = create_single_batch([dataset[i] for i in test_indices])\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    # Create DataLoaders\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "    # # val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = GCN_Graph(args.input_dim, output_dim=1, dropout=args.dropout).to(device)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args.lr) # weight_decay=args.weight_decay\n",
    "    # print out model parameter values\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print (name, param.data)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    val_accs = 0\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    print(args.epochs)\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epoch\"):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        train_dataset.to(device)\n",
    "        pred = model(train_dataset)\n",
    "        pred = pred.squeeze()\n",
    "        label = train_dataset.y.float().to(device)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss = loss.item()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        # Test accuracy\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_dataset, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print (name, param.data)\n",
    "        \n",
    "    return test_accs, val_accs, losses, best_model, best_acc\n",
    "\n",
    "def test(test_dataset, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_dataset).max(dim=1)[1]\n",
    "        label = test_dataset.y\n",
    "    correct += pred.eq(label).sum().item()\n",
    "    total += test_dataset.num_graphs\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "args = {'batch_size': 32, \n",
    "        'dropout': 0.9, \n",
    "        'epochs': 500, \n",
    "        'input_dim' : 7,\n",
    "        'opt': 'adam', \n",
    "        'opt_scheduler': 'none', \n",
    "        'opt_restart': 0, \n",
    "        'weight_decay': 5e-3, \n",
    "        'lr': 0.01}\n",
    "\n",
    "args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "gnn_node.convs.0.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "gnn_node.convs.0.lin.weight tensor([[-0.2143, -0.0292,  0.3851,  0.1386,  0.2862, -0.0400, -0.1789],\n",
      "        [-0.0652,  0.2892,  0.2279,  0.3867,  0.1750, -0.1871, -0.1103],\n",
      "        [ 0.3309,  0.0989, -0.0203, -0.0468,  0.3721,  0.3509,  0.3145],\n",
      "        [ 0.2211,  0.0352, -0.3913, -0.0823, -0.3685, -0.0131,  0.2032],\n",
      "        [ 0.0261, -0.3209, -0.3676,  0.1979, -0.1732,  0.3205, -0.2732],\n",
      "        [-0.0190,  0.3417,  0.0575,  0.1929,  0.1313,  0.2107, -0.1593],\n",
      "        [-0.3044,  0.3321,  0.0169, -0.2139,  0.0971, -0.3066, -0.2264],\n",
      "        [-0.1424,  0.0834,  0.2776,  0.0528, -0.2649, -0.3408, -0.0888],\n",
      "        [ 0.0949,  0.3042,  0.2207, -0.2277, -0.0555, -0.0431,  0.0184],\n",
      "        [-0.1153,  0.0295,  0.3091,  0.0306,  0.0392,  0.0877, -0.3406],\n",
      "        [ 0.1571, -0.0827,  0.1812, -0.0726,  0.3594,  0.2747, -0.2792],\n",
      "        [-0.3609,  0.0878,  0.3293, -0.0817,  0.2344, -0.1436,  0.3435],\n",
      "        [-0.2978,  0.2991, -0.0047, -0.2303,  0.0172,  0.2924, -0.1858],\n",
      "        [-0.2543,  0.0973,  0.2065, -0.3290,  0.0229, -0.1007, -0.1300],\n",
      "        [-0.2700, -0.0367,  0.3054, -0.1624, -0.3600, -0.1411, -0.1386],\n",
      "        [-0.2423,  0.1546,  0.0109, -0.1516, -0.0391,  0.1100, -0.2181],\n",
      "        [ 0.3605,  0.0768,  0.3436,  0.1180,  0.0920, -0.2319,  0.3696],\n",
      "        [ 0.3163, -0.1856,  0.1094, -0.2547,  0.0643,  0.1299, -0.3455],\n",
      "        [ 0.1349,  0.0519, -0.1650,  0.3231,  0.1356, -0.1842, -0.0381],\n",
      "        [ 0.3445, -0.1684, -0.0850,  0.0700, -0.3709, -0.1903, -0.3235],\n",
      "        [-0.1926,  0.1622,  0.3290,  0.1048, -0.1075,  0.0260, -0.0467],\n",
      "        [-0.3379, -0.1804,  0.2568, -0.3861,  0.1052,  0.3761,  0.2691],\n",
      "        [-0.2297,  0.0524, -0.1486,  0.0570,  0.3829,  0.2059,  0.2054],\n",
      "        [ 0.1451,  0.0373, -0.1692, -0.0654,  0.2248,  0.2157,  0.2360],\n",
      "        [ 0.2188, -0.0159,  0.0756,  0.1959,  0.1433, -0.3917, -0.2594],\n",
      "        [ 0.3254, -0.3835, -0.0122, -0.3009,  0.2457,  0.1841, -0.2444],\n",
      "        [-0.0208,  0.2102,  0.3766,  0.1657,  0.0281, -0.1093,  0.1264],\n",
      "        [ 0.1305, -0.3458, -0.0103, -0.3048,  0.1212, -0.2990,  0.3339],\n",
      "        [ 0.0424,  0.0244, -0.2400,  0.2597, -0.0416, -0.3681,  0.3281],\n",
      "        [-0.1655,  0.0864, -0.3582, -0.3326,  0.2907,  0.2165, -0.1413],\n",
      "        [ 0.0099, -0.2712, -0.3449,  0.1634, -0.3861,  0.1557,  0.2761],\n",
      "        [-0.1744, -0.0861, -0.1986,  0.0383,  0.0087,  0.3767,  0.3472]])\n",
      "gnn_node.convs.1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "gnn_node.convs.1.lin.weight tensor([[ 5.6146e-02,  1.0935e-01, -1.2952e-01,  ..., -2.4925e-01,\n",
      "          2.3964e-01, -1.1082e-01],\n",
      "        [-2.0956e-01,  2.3459e-01,  7.1026e-02,  ...,  1.7508e-01,\n",
      "          7.8742e-02, -2.1802e-01],\n",
      "        [ 1.8216e-01, -7.1990e-02,  3.1112e-02,  ...,  2.1331e-01,\n",
      "          4.7974e-02, -3.2631e-02],\n",
      "        ...,\n",
      "        [ 3.1578e-03,  2.7268e-01,  5.6335e-02,  ..., -1.2913e-02,\n",
      "         -2.3842e-01, -2.0077e-01],\n",
      "        [ 1.5630e-01,  8.7422e-02, -1.3727e-01,  ..., -9.3281e-06,\n",
      "         -2.7104e-01, -6.3282e-02],\n",
      "        [ 1.6761e-01, -2.5180e-01,  3.0759e-02,  ...,  4.1224e-02,\n",
      "          1.5173e-02, -2.1919e-01]])\n",
      "gnn_node.convs.2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "gnn_node.convs.2.lin.weight tensor([[-0.0675,  0.1967, -0.1058,  ..., -0.0476,  0.0614, -0.1719],\n",
      "        [ 0.0371, -0.2271,  0.1762,  ..., -0.1676,  0.2294, -0.1215],\n",
      "        [ 0.1197,  0.1399, -0.1958,  ...,  0.0204,  0.1254, -0.0877],\n",
      "        ...,\n",
      "        [-0.1988,  0.0161, -0.0673,  ..., -0.0106,  0.2069, -0.0999],\n",
      "        [-0.1442,  0.0243,  0.1355,  ..., -0.1303,  0.2303,  0.1939],\n",
      "        [-0.1672,  0.2207, -0.0435,  ..., -0.1342, -0.1271, -0.1440]])\n",
      "post_mp.0.weight tensor([[-0.0195,  0.1215,  0.0817,  ...,  0.0522,  0.0297,  0.1212],\n",
      "        [ 0.1012,  0.1006,  0.0987,  ...,  0.0550, -0.0140, -0.0788],\n",
      "        [ 0.0046, -0.0275, -0.0867,  ...,  0.0810,  0.0808, -0.0498],\n",
      "        ...,\n",
      "        [ 0.0721, -0.1126,  0.0991,  ..., -0.1136,  0.1182,  0.0097],\n",
      "        [ 0.0050, -0.1070, -0.0170,  ...,  0.0279,  0.0114,  0.0111],\n",
      "        [ 0.1160, -0.0040, -0.0958,  ..., -0.0429,  0.0614, -0.0332]])\n",
      "post_mp.0.bias tensor([-0.0857,  0.0223,  0.0817, -0.1069,  0.0845,  0.0006, -0.0332, -0.0133,\n",
      "         0.0449,  0.0246,  0.0167,  0.0712,  0.0804,  0.0580,  0.0780,  0.1096,\n",
      "         0.0212,  0.0979, -0.0305,  0.0450,  0.0462,  0.0304,  0.0510,  0.0254,\n",
      "        -0.1144, -0.1242, -0.0926, -0.0755, -0.0322,  0.0704,  0.0282, -0.0096])\n",
      "post_mp.2.weight tensor([[ 0.1354, -0.1491,  0.0374,  0.1436,  0.0524,  0.1726,  0.1087, -0.1522,\n",
      "         -0.0465,  0.0738,  0.0476, -0.0840,  0.1564, -0.1156, -0.1159, -0.0571,\n",
      "          0.1616, -0.0047, -0.0651, -0.1018,  0.1240, -0.0939,  0.0702,  0.0004,\n",
      "         -0.1194,  0.1013, -0.0768, -0.1017,  0.1262,  0.0369,  0.0872, -0.1496]])\n",
      "post_mp.2.bias tensor([0.0734])\n",
      "500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa65412f3a4f5bbf70ca4faca3f5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?Epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn_node.convs.0.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "gnn_node.convs.0.lin.weight tensor([[-0.2143, -0.0292,  0.3851,  0.1386,  0.2862, -0.0400, -0.1789],\n",
      "        [-0.0652,  0.2892,  0.2279,  0.3867,  0.1750, -0.1871, -0.1103],\n",
      "        [ 0.3309,  0.0989, -0.0203, -0.0468,  0.3721,  0.3509,  0.3145],\n",
      "        [ 0.2211,  0.0352, -0.3913, -0.0823, -0.3685, -0.0131,  0.2032],\n",
      "        [ 0.0261, -0.3209, -0.3676,  0.1979, -0.1732,  0.3205, -0.2732],\n",
      "        [-0.0190,  0.3417,  0.0575,  0.1929,  0.1313,  0.2107, -0.1593],\n",
      "        [-0.3044,  0.3321,  0.0169, -0.2139,  0.0971, -0.3066, -0.2264],\n",
      "        [-0.1424,  0.0834,  0.2776,  0.0528, -0.2649, -0.3408, -0.0888],\n",
      "        [ 0.0949,  0.3042,  0.2207, -0.2277, -0.0555, -0.0431,  0.0184],\n",
      "        [-0.1153,  0.0295,  0.3091,  0.0306,  0.0392,  0.0877, -0.3406],\n",
      "        [ 0.1571, -0.0827,  0.1812, -0.0726,  0.3594,  0.2747, -0.2792],\n",
      "        [-0.3609,  0.0878,  0.3293, -0.0817,  0.2344, -0.1436,  0.3435],\n",
      "        [-0.2978,  0.2991, -0.0047, -0.2303,  0.0172,  0.2924, -0.1858],\n",
      "        [-0.2543,  0.0973,  0.2065, -0.3290,  0.0229, -0.1007, -0.1300],\n",
      "        [-0.2700, -0.0367,  0.3054, -0.1624, -0.3600, -0.1411, -0.1386],\n",
      "        [-0.2423,  0.1546,  0.0109, -0.1516, -0.0391,  0.1100, -0.2181],\n",
      "        [ 0.3605,  0.0768,  0.3436,  0.1180,  0.0920, -0.2319,  0.3696],\n",
      "        [ 0.3163, -0.1856,  0.1094, -0.2547,  0.0643,  0.1299, -0.3455],\n",
      "        [ 0.1349,  0.0519, -0.1650,  0.3231,  0.1356, -0.1842, -0.0381],\n",
      "        [ 0.3445, -0.1684, -0.0850,  0.0700, -0.3709, -0.1903, -0.3235],\n",
      "        [-0.1926,  0.1622,  0.3290,  0.1048, -0.1075,  0.0260, -0.0467],\n",
      "        [-0.3379, -0.1804,  0.2568, -0.3861,  0.1052,  0.3761,  0.2691],\n",
      "        [-0.2297,  0.0524, -0.1486,  0.0570,  0.3829,  0.2059,  0.2054],\n",
      "        [ 0.1451,  0.0373, -0.1692, -0.0654,  0.2248,  0.2157,  0.2360],\n",
      "        [ 0.2188, -0.0159,  0.0756,  0.1959,  0.1433, -0.3917, -0.2594],\n",
      "        [ 0.3254, -0.3835, -0.0122, -0.3009,  0.2457,  0.1841, -0.2444],\n",
      "        [-0.0208,  0.2102,  0.3766,  0.1657,  0.0281, -0.1093,  0.1264],\n",
      "        [ 0.1305, -0.3458, -0.0103, -0.3048,  0.1212, -0.2990,  0.3339],\n",
      "        [ 0.0424,  0.0244, -0.2400,  0.2597, -0.0416, -0.3681,  0.3281],\n",
      "        [-0.1655,  0.0864, -0.3582, -0.3326,  0.2907,  0.2165, -0.1413],\n",
      "        [ 0.0099, -0.2712, -0.3449,  0.1634, -0.3861,  0.1557,  0.2761],\n",
      "        [-0.1744, -0.0861, -0.1986,  0.0383,  0.0087,  0.3767,  0.3472]])\n",
      "gnn_node.convs.1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "gnn_node.convs.1.lin.weight tensor([[ 5.6146e-02,  1.0935e-01, -1.2952e-01,  ..., -2.4925e-01,\n",
      "          2.3964e-01, -1.1082e-01],\n",
      "        [-2.0956e-01,  2.3459e-01,  7.1026e-02,  ...,  1.7508e-01,\n",
      "          7.8742e-02, -2.1802e-01],\n",
      "        [ 1.8216e-01, -7.1990e-02,  3.1112e-02,  ...,  2.1331e-01,\n",
      "          4.7974e-02, -3.2631e-02],\n",
      "        ...,\n",
      "        [ 3.1578e-03,  2.7268e-01,  5.6335e-02,  ..., -1.2913e-02,\n",
      "         -2.3842e-01, -2.0077e-01],\n",
      "        [ 1.5630e-01,  8.7422e-02, -1.3727e-01,  ..., -9.3281e-06,\n",
      "         -2.7104e-01, -6.3282e-02],\n",
      "        [ 1.6761e-01, -2.5180e-01,  3.0759e-02,  ...,  4.1224e-02,\n",
      "          1.5173e-02, -2.1919e-01]])\n",
      "gnn_node.convs.2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "gnn_node.convs.2.lin.weight tensor([[-0.0675,  0.1967, -0.1058,  ..., -0.0476,  0.0614, -0.1719],\n",
      "        [ 0.0371, -0.2271,  0.1762,  ..., -0.1676,  0.2294, -0.1215],\n",
      "        [ 0.1197,  0.1399, -0.1958,  ...,  0.0204,  0.1254, -0.0877],\n",
      "        ...,\n",
      "        [-0.1988,  0.0161, -0.0673,  ..., -0.0106,  0.2069, -0.0999],\n",
      "        [-0.1442,  0.0243,  0.1355,  ..., -0.1303,  0.2303,  0.1939],\n",
      "        [-0.1672,  0.2207, -0.0435,  ..., -0.1342, -0.1271, -0.1440]])\n",
      "post_mp.0.weight tensor([[-0.0195,  0.1215,  0.0817,  ...,  0.0522,  0.0297,  0.1212],\n",
      "        [ 0.1012,  0.1006,  0.0987,  ...,  0.0550, -0.0140, -0.0788],\n",
      "        [ 0.0046, -0.0275, -0.0867,  ...,  0.0810,  0.0808, -0.0498],\n",
      "        ...,\n",
      "        [ 0.0721, -0.1126,  0.0991,  ..., -0.1136,  0.1182,  0.0097],\n",
      "        [ 0.0050, -0.1070, -0.0170,  ...,  0.0279,  0.0114,  0.0111],\n",
      "        [ 0.1160, -0.0040, -0.0958,  ..., -0.0429,  0.0614, -0.0332]])\n",
      "post_mp.0.bias tensor([-0.0857,  0.0223,  0.0817, -0.1069,  0.0845,  0.0006, -0.0332, -0.0133,\n",
      "         0.0449,  0.0246,  0.0167,  0.0712,  0.0804,  0.0580,  0.0780,  0.1096,\n",
      "         0.0212,  0.0979, -0.0305,  0.0450,  0.0462,  0.0304,  0.0510,  0.0254,\n",
      "        -0.1144, -0.1242, -0.0926, -0.0755, -0.0322,  0.0704,  0.0282, -0.0096])\n",
      "post_mp.2.weight tensor([[ 0.1354, -0.1491,  0.0374,  0.1436,  0.0524,  0.1726,  0.1087, -0.1522,\n",
      "         -0.0465,  0.0738,  0.0476, -0.0840,  0.1564, -0.1156, -0.1159, -0.0571,\n",
      "          0.1616, -0.0047, -0.0651, -0.1018,  0.1240, -0.0939,  0.0702,  0.0004,\n",
      "         -0.1194,  0.1013, -0.0768, -0.1017,  0.1262,  0.0369,  0.0872, -0.1496]])\n",
      "post_mp.2.bias tensor([0.0734])\n",
      "Maximum test set accuracy: 0.39473684210526316\n",
      "Minimum loss: 0.6931471228599548\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "num_graphs = len(dataset)\n",
    "\n",
    "# Define split percentages\n",
    "train_percentage = 0.8\n",
    "val_percentage = 0.0\n",
    "\n",
    "# Calculate split sizes\n",
    "train_size = int(num_graphs * train_percentage)\n",
    "val_size = int(num_graphs * val_percentage)\n",
    "test_size = num_graphs - train_size - val_size\n",
    "\n",
    "# Create shuffled indices\n",
    "indices = np.random.permutation(num_graphs)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "test_accs, val_accs, losses, best_model, best_acc = train(dataset, args, train_indices, val_indices, test_indices)\n",
    "\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# # Run test for our best model to save the predictions!\n",
    "# test(test_loader, best_model)\n",
    "# print()\n",
    "\n",
    "plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\")\n",
    "plt.plot(test_accs, label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54a8a4b80f4f70c84f31aca617cda6666e967fece7258fbe31fedfa48a0f63f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
